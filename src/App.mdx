import TokenizerTriangleApp from "./TokenizerTriangleApp";
import {
  DemoPanel,
  FooterNote,
  GuideLayout,
  GuideList,
  HighlightPanel,
  Hero,
  PillarGrid,
  Section,
} from "./components/GuideComponents";

<GuideLayout>
  <Hero
    title="Speech Tokenizers"
    linkHref="https://systemdevart.github.io/speech-tokenizers/"
    linkLabel="https://systemdevart.github.io/speech-tokenizers/"
  >
    This guide with interactive demo is best viewed at the hosted site.
  </Hero>

  <HighlightPanel title="TL;DR">
    <p>
      This is a highly opinionated guide on speech tokenizers based on our experience at{" "}
      <a href="http://dubformer.ai" target="_blank" rel="noreferrer">
        dubformer.ai
      </a>{" "}
      building voice cloning and TTS models. If you're just starting out, we recommend the linguistic tokenizer S3 or
      the semantic tokenizer MaskGCT.
    </p>
  </HighlightPanel>

  <Section title="Introduction">
    <p>
      This guide provides a short overview of modern (and a few classic) speech tokenizers. We focus on tokenizers for
      downstream TTS models—not general audio or music tokenization. The guide divides modern tokenizers into three
      categories: acoustic (low-level waveform information), semantic (pseudo-phonetic), and linguistic (high-level text
      information used as a training signal). Start by reviewing the diagram and table below, then explore the detailed
      description of each tokenizer group.
    </p>
    <p>
      The interactive triangle lets you compare models quickly, while the filter chips and search highlight the design
      choices that matter for production voice systems: bitrate, quantization strategy, supervised signals, and
      reconstruction objectives.
    </p>
  </Section>

  <DemoPanel
    title="Explore the landscape"
    eyebrow="Interactive reference"
    description="Hover a point to preview details, click to lock, toggle groups, or search for architectures and objectives."
  >
    <TokenizerTriangleApp embedded />
  </DemoPanel>

  <Section title="What the clusters mean">
    <p>
      The triangle corners correspond to the signal a tokenizer leans on most. Acoustic codecs chase waveform fidelity,
      semantic codecs distill self-supervised speech models, and linguistic codecs rely on text-form supervision to
      capture phonemes, syllables, or grapheme-level units.
    </p>
    <PillarGrid />
  </Section>

  <Section title="How to read the cards">
    <GuideList>
      <li>
        <span>Frame rate:</span> Token cadence in Hz – lower rates integrate more easily with LLMs but may lose temporal
        detail.
      </li>
      <li>
        <span>Quantization:</span> RVQ, SVQ, FSQ, and hybrids trade reconstruction fidelity for compact tokens.
      </li>
      <li>
        <span>Auxiliary signals:</span> Whether the encoder receives speaker embeddings, self-supervised distillation,
        or diffusion-style supervision.
      </li>
    </GuideList>
  </Section>

  <FooterNote>
    Want to dig deeper? Open the hosted demo for side-by-side comparison, export a tokenizer subset for your lab notes,
    or fork this project to plug in your own models.
  </FooterNote>
</GuideLayout>
